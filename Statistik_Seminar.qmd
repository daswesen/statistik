---
title: "Statistik für die Angewandte Forschung"
format: pptx
author: "Dr. Tom Alby"
---

# Sitzung 1: Einführung & Organisatorisches

## Ziele dieser Veranstaltung

-   Werkzeuge für Datenanalyse in der Bachelor-Arbeit
-   Grundkenntnisse Statistik, nicht nur für die Uni

## Was Sie erwartet

-   Einführung in R und RStudio
-   Datenimport und -aufbereitung
-   Explorative Datenanalyse
-   Datenvisualisierung
-   Wahrscheinlichkeitsrechnung
-   Lineare Regression
-   Hypothesentests
-   Korrelationen

## Erwartungen und Interaktion

-   **Slido:** Interaktive Beteiligung über [Slido](https://app.sli.do)
    -   16 Uhr: #68430031
    -   18 Uhr: #68430032

## Welche Note kann ich erwarten?

![](images/grade-distribution.png)

## Rules of Working

-   **Fragen:** Bitte unterbrechen Sie mich für Fragen
-   **Kontakt:** [tom\@alby.de](mailto:tom@alby.de)
-   **Moodle:** Einschreibeschlüssel: iLoveStats!
-   **Materialien:** <https://alby.link/statistik>
-   **Teilnahme:** Präsenzpflicht, in Ausnahmefällen auch online via Zoom (HU), Aufzeichnungen verfügbar

## Prüfungsleistung

-   **Prüfungsleistung:** Hausarbeit mit einer Bearbeitungszeit von sechs Wochen
-   **Keine Gruppenarbeit:** Im Kurs ja, für die Hausarbeit nein
-   **ChatGPT & Co:** Erlaubt, aber...
-   Bitte kein **bedeutungsloser Wohlklang**

## Wie sieht die Hausarbeit aus?

-   **Sie** suchen sich einen Datensatz aus und **holen** sich das **Ok**
-   Hausarbeit in einem speziellen **Dateiformat**, das im Kurs vermittelt wird
-   Hausarbeit wird **während des Semesters** vorbereitet
-   Wöchentliche Besprechung im Seminar
-   Möglichkeit der **Probeabgabe**
-   **Bewertungskriterien** sind online

## Letzte Fragen?

## Unterstützung für Studierende

-   Studentische Seelsorge, rund um die Uhr: 040 41170411
-   [Studierendenberatung\@haw-hamburg.de](mailto:Studierendenberatung@haw-hamburg.de)
-   [krisenteam\@haw-hamburg.de](mailto:krisenteam@haw-hamburg.de)
-   Psychologische Sprechzeit ohne Anmeldung (Dienstags 14-15 Uhr): +49 152 247 44 063
-   Weitere Infos: [HAW Beratung](https://www.haw-hamburg.de/beratung/)

## Let's start!

![](images/marketoonist-big-data.png)

## Was sind Daten?

## Was sind Daten?

-   *Daten*
    -   Roh, ungeordnet, noch nicht interpretiert
    -   Bestehen aus Zahlen, Zeichen, Fakten ohne Kontext
-   *Informationen*
    -   Verarbeitete, kontextualisierte Daten
    -   Haben Bedeutung und Relevanz für eine Fragestellung
    -   **Daten + Kontext = Information**

## DIKW-Pyramide

![](images/dikw-pyramid-removebg-preview.png)

## Toms Pyramide

![](images/tattoo.png)

## Daten oder Informationen?

![](images/daten-oder-informationen.png)

## Skalenniveau

| Statistische Einheit / Merkmalsträger | Merkmal | Ausprägung | Typ / Skala | Erläuterung |
|---------------|---------------|---------------|---------------|---------------|
| n1 | Haarfarbe | Rot | Kategorial / nominalskaliert | Keine Abstände |
| n1 | Postleitzahl | 22703 | Numerisch / nominalskaliert | Keine Abstände |
| n1 | Schulabschluss | Abitur | Kategorial / ordinalskaliert | willkürliche Abstände |
| Sport-Team A | Tabellenplatz | 7 | Numerisch / ordinalskaliert | willkürliche Abstände |
| n1 | Körpertemperatur | 36,5°C. | Numerisch / intervallskaliert | kein Nullpunkt |
| n1 | Alter | 22 | Numerisch / verhältnisskal. | keine Negativwerte |

## Skalenniveau am Beispiel

| title | year | budget | avg_vote |
|------------------|------------------|------------------|------------------|
| Miss Jerry | 1894 | NA | 5.9 |
| The Story of the Kelly Gang | 1906 | \$ 2250 | 6.1 |
| Den sorte drøm | 1911 | NA | 5.8 |
| Cleopatra | 1912 | \$ 45000 | 5.2 |
| L'Inferno | 1911 | NA | 7.0 |
| From the Manger to the Cross; or, Jesus of Nazareth | 1912 | NA | 5.7 |
| Madame DuBarry | 1919 | NA | 6.8 |
| Quo Vadis? | 1913 | ITL 45000 | 6.2 |
| Independenta Romaniei | 1912 | ROL 400000 | 6.7 |
| Richard III | 1912 | \$ 30000 | 5.5 |

## Analytische Konzepte

| Phase & Leitfrage | Statistisches Äquivalent | Beispiel‑Methoden / Tools |
|------------------------|------------------------|------------------------|
| **Rückblick – Was ist passiert?** | **Deskriptive Statistik** | Lage‑ & Streuungsmaße, Häufigkeiten, Balken‑/Liniendiagramme |
| **Erkenntnis – Warum passiert es?** | **Explorative & inferenzielle Statistik** | Korrelation, t‑Test, ANOVA, χ²‑Test, Konfidenz­intervalle, Regressionsdiagnostik |
| **Vorausschau – Was wird passieren?** | **Prognostische (inferenzielle) Modelle** | Lineare/Logistische Regression, Random Forest, Zeitreihen‑Forecasts, Konfidenz­intervalle für Prognosen |
| **Vorausschau – Was muss passieren, damit es eintritt?** | **Präskriptive (inferenzielle) Verfahren** | Simulation (Monte‑Carlo), Lineare/Ganzzahl‑Optimierung, Entscheidungsbäume mit Unsicherheits‑Berücksichtigung |

## Wo finde ich Daten?

-   Kaggle
-   Statistisches Bundesamt
-   ...

## Warum nutzen wir R?

![](images/r-vs-python.png)

## R, Python, SPSS, ...

-   SPSS
-   SAS
-   Python
-   Scala
-   Julia
-   PSPP
-   R

## Die Programmiersprache R

-   R: Statistische Programmiersprache und Umgebung
-   Nachfolger von S (Unnützes Wissen, Teil 1)
-   Großer Vorteil: R ist von Statistikern für Statistiker
-   Großer Nachteil: R ist von Statistikern für Statistiker
-   Open Source
-   Packages (Libraries) erweitern den Umfang

## Die IDE RStudio

-   "Wenn R ein Pferd ist, dann ist RStudio der Sattel."
-   Entwickelt von RStudio PBC, heute Posit PBC
-   Kostenlos für den Heimgebrauch

## Die Arbeitsumgebung

-   Sozusagen der “Arbeitsspeicher”
-   Enthält nicht nur Objekte, sondern auch Packages
-   Häufige Fehlerursache: -- Man geht davon aus, dass etwas geladen ist, ist es aber nicht. -- Man geht davon aus, dass etwas nicht geladen ist, ist es aber.
-   R lädt alles (!) in den Arbeitsspeicher.

## Die ersten Schritte mit R

-   R als Taschenrechner
-   Objekt- und Funktionsstruktur in R
-   Zuordnung über `<-`
-   Hilfe über `?FUNKTIONSNAME`

## Funktionen

-   Funktionsstruktur: `funktionsname(Parameter)`
-   Beispiele: `plot()`, `summary()`, `str()`

## Datentypen in R

-   Character
-   Numerisch: integer und double
-   Factor
-   Datum
-   Dynamic typing und Bedeutung korrekter Datentypen

## Datenstrukturen in R

![](images/data-structures.png)

## Datenstrukturen in R

-   Vektor: Liste mit gleichen Datentypen
-   Dataframe: Tabelle mit Vektoren als Spalten
-   Liste: verschiedene Datentypen
-   Matrix: Tabelle nur mit gleichen Datentypen
-   Array: Dreidimensionale Matrix

## Dataframes

-   Zentrales Konzept in R
-   Struktur: Zeilen für Observationen, Spalten für Attribute
-   Zugriff auf Spalten über Dollarzeichen (`$`)

# Sitzung 2: Deskriptive Statistik I

## Let's talk about money

![](images/einkommen.jpeg)

## Wie könnte man die Einkommensverteilung beschreiben?

![](images/einkommensschichtung.png)

## Histogramm

-   Was: Grafische Darstellung der Häufigkeitsverteilung in Klassen
-   Bestandteile: X-Achse (Werte), Y-Achse (Häufigkeit), Balken (Klassenhäufigkeit)
-   Wozu: Schneller Überblick über Datenverteilung und -muster
-   Interpretation: Form (symmetrisch/schief), Modalität (uni-/multimodal), Ausreißer
-   Wichtig: Klassenanzahl beeinflusst sichtbare Form
-   In R: `hist(data, breaks = 10)`

## Warum sind statistische Kennzahlen wichtig?

## Daten zusammenfassen und vereinfachen

-   **Reduktion von Komplexität**: Große Datenmengen werden durch wenige Werte beschreibbar
-   **Kommunikation**: Leichtere Vermittlung von Erkenntnissen an Nicht-Statistiker
-   **Vergleichbarkeit**: Unterschiedliche Datensätze werden vergleichbar

## Entscheidungsgrundlage in vielen Anwendungsbereichen

-   **Wirtschaft**: Grundlage für Investitionsentscheidungen und Marktanalysen
-   **Medizin**: Interpretation von Studienergebnissen und diagnostischen Tests
-   **Politik**: Faktenbasis für gesellschaftliche und wirtschaftliche Maßnahmen
-   **Wissenschaft**: Prüfung von Hypothesen und Interpretation von Experimenten

## Typische Anwendungsfälle

| Kennzahl | Wichtig für... | Beispielanwendung |
|------------------------|------------------------|------------------------|
| **Mittelwert** | Durchschnittswerte, Summenbetrachtungen | Durchschnittseinkommen einer Region |
| **Median** | Typische Werte, robust gegen Ausreißer | Typisches Haushaltseinkommen |
| **Standardabweichung** | Streuung, Qualitätskontrolle | Produktionsgenauigkeit in der Fertigung |
| **Quartile/Boxplot** | Verteilungsform, Ausreißererkennung | Identifikation von Ungleichheiten |
| **Schiefe/Wölbung** | Abweichungen von der Normalverteilung | Risikobewertung im Finanzwesen |

## Von der Beschreibung zur Inferenz

-   Deskriptive Statistik als Grundlage für **statistische Tests**
-   Möglichkeit der **Stichprobenerhebung** statt Vollerhebung
-   Basis für **statistische Modellierung** und **Vorhersagen**

## Erkennung von Mustern und Besonderheiten

-   **Ausreißer**: Potentielle Fehler oder besondere Fälle
-   **Trends**: Entwicklungen über Zeit
-   **Gruppenunterschiede**: Systematische Variation zwischen Teilgruppen

## Typisch / ungewöhnlich / unmöglich

![](images/hadlum.png)

## Weitere Beispiele

| Anwendungsgebiet | Kennzahlen | Nutzen |
|------------------------|------------------------|------------------------|
| **Medizin** | Referenzwerte (z.B. Blutdruck, BMI) | Diagnostik, Risikoeinschätzung |
| **Finanzmarkt** | Volatilität (Standardabweichung) | Risikobewertung von Anlagen |
| **Qualitätssicherung** | Prozessfähigkeitsindizes (basierend auf σ) | Optimierung von Produktionsprozessen |
| **Sozialforschung** | Gini-Koeffizient (basierend auf Verteilung) | Ungleichheitsmaße in der Gesellschaft |
| **Meteorologie** | Durchschnittstemperaturen + Extremwerte | Klimaforschung, Wettervorhersage |

## Lagemaße

![](images/mean-mode-median.png)

## Lagemaße

-   Mean bezeichnet das arithmetische Mittel, häufig als „Durchschnitt" bezeichnet: mean()
-   Neben dem arithmetischen Mittel existieren weitere Mittelwerte, zum Beispiel das geometrische Mittel
-   Der Median ist weniger anfällig für Ausreißer: median()
-   Der Modalwert kann auch mit kategorialen Variablen umgehen
-   In R existiert kein Befehl für den Modalwert ☹

## Merke

![](images/meme-durchschnitt.png)

## It really happened

![](images/clinton-average.png)

## Auch die MOPO...

![](images/Bildschirmfoto%202025-05-09%20um%2011.34.35.png)

## Streuungsmaße

## Spannweite (Range)

-   Die Spannweite ist der einfachste Streuungsparameter
-   Berechnung: Maximum - Minimum
-   Sehr anfällig für Ausreißer
-   In R: `diff(range(data))` oder `max(data) - min(data)`

## Interquartilsabstand (IQR)

-   Robusteres Streuungsmaß, das Ausreißer besser berücksichtigt
-   Berechnung: 75%-Quartil minus 25%-Quartil (Q3 - Q1)
-   Enthält die mittleren 50% der Daten
-   In R: `IQR(data)` oder `quantile(data, 0.75) - quantile(data, 0.25)`

## Varianz

-   Durchschnittliche quadratische Abweichung vom Mittelwert
-   Berechnung für Grundgesamtheit: $$\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2$$
-   Berechnung für Stichprobe (korrigierte Varianz): $$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$
-   Einheit: Quadrat der Einheit der Ursprungsdaten
-   In R: `var(data)`

## Standardabweichung

![](images/Standard_deviation_diagram.svg.png)

## Standardabweichung

-   Wurzel aus der Varianz: Dieselbe Einheit wie die Ursprungsdaten
-   Berechnung für Grundgesamtheit: $$\sigma = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2}$$
-   Berechnung für Stichprobe: $$s = \sqrt{\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2}$$
-   In R: `sd(data)`

## Interpretation der Standardabweichung im Kontext der Normalverteilung

-   Bei normalverteilten Daten liegen ca. 68% der Werte innerhalb von μ ± σ (Mittelwert ± 1 Standardabweichung)
-   Ca. 95% der Werte liegen innerhalb von μ ± 2σ
-   Ca. 99,7% der Werte liegen innerhalb von μ ± 3σ ("Drei-Sigma-Regel")

## Formparameter

![](images/3-s2.0-B9780128207178000087-f05-03-9780128207178.jpg)

## Schiefe (Skewness)

-   Misst die Asymmetrie einer Verteilung
-   Positive Schiefe: rechtsschiefe Verteilung (längerer "Schwanz" nach rechts)
-   Negative Schiefe: linksschiefe Verteilung (längerer "Schwanz" nach links)
-   Schiefe von 0: symmetrische Verteilung (z.B. Normalverteilung)
-   In R: `skewness(data)` aus dem Paket **moments** oder **e1071**

## Wölbung (Kurtosis)

-   Misst die "Spitzheit" bzw. "Flachheit" einer Verteilung
-   Normalverteilung hat eine Kurtosis von 3 (manchmal als Exzess-Kurtosis = 0 dargestellt)
-   Hohe Kurtosis (\>3): spitze Verteilung mit "schweren Enden" (mehr Extremwerte)
-   Niedrige Kurtosis (\<3): flache Verteilung mit "leichten Enden" (weniger Extremwerte)
-   In R: `kurtosis(data)` aus dem Paket **moments** oder **e1071**

## Positionskennwerte

## Quantile und Perzentile

-   Teile die Daten in gleich große Teile
-   Median = 0,5-Quantil (50%-Perzentil)
-   Quartile teilen die Daten in vier gleiche Teile:
    -   Q1 = 0,25-Quantil (25%-Perzentil)
    -   Q2 = 0,5-Quantil (50%-Perzentil) = Median
    -   Q3 = 0,75-Quantil (75%-Perzentil)
-   In R: `quantile(data, probs = c(0.25, 0.5, 0.75))` für Quartile

## Boxplot

![](images/Elements_of_a_boxplot.svg.png)

## Boxplot

-   Visuelle Darstellung von Quartilen und Ausreißern
-   Box: Interquartilsabstand (IQR) von Q1 bis Q3
-   Mittellinie: Median (Q2)
-   Whiskers: Typischerweise bis max. 1,5 × IQR von der Box
-   Punkte außerhalb der Whiskers: potenzielle Ausreißer
-   In R: `boxplot(data)`

\#![](images/boxplot-explained.png)

## Wie macht man das in R?

| Kennzahl | R‑Befehl | Kurzbeschreibung |
|------------------------|------------------------|------------------------|
| Mittelwert | `mean(data)` | Durchschnitt aller Werte |
| Median | `median(data)` | Zentralwert (50 %-Quantil) |
| Minimum | `min(data)` | Kleinster Wert |
| Maximum | `max(data)` | Größter Wert |
| Spannweite | `diff(range(data))` | Abstand = Max − Min |
| Standardabweichung | `sd(data)` | Ø‑Abweichung vom Mittelwert |
| Varianz | `var(data)` | Quadrat der Standardabweichung |
| Schiefe | `skewness(data)` *(moments/e1071)* | Asymmetrie der Verteilung |
| Wölbung (Kurtosis) | `kurtosis(data)` *(moments/e1071)* | "Spitzheit" der Verteilung |
| Quantile | `quantile(data, c(.25,.5,.75))` | 25 %, 50 %, 75 %-Werte (Quartile) |

## Übung mit dem Datensatz iris

| Kennzahl           | R‑Befehl            | Kurzbeschreibung               |
|--------------------|---------------------|--------------------------------|
| Mittelwert         | `mean(data)`        | Durchschnitt aller Werte       |
| Median             | `median(data)`      | Zentralwert (50 %-Quantil)     |
| Minimum            | `min(data)`         | Kleinster Wert                 |
| Maximum            | `max(data)`         | Größter Wert                   |
| Spannweite         | `diff(range(data))` | Abstand = Max − Min            |
| Standardabweichung | `sd(data)`          | Ø‑Abweichung vom Mittelwert    |
| Varianz            | `var(data)`         | Quadrat der Standardabweichung |

# Sitzung 3: Deskriptive Statistik II & Datenmanipulation

## Typischer Ablauf

![](images/CRISP-DM_Process_Diagram.png)

## Tidyverse

-   Das Tidyverse-Paket von Hadley Wickham
-   Code the way you think
-   Enthält Packages wie dplyr und readr

## Packages installieren und laden

-   Packages als Erweiterungen zu base R
-   Installation über Tools =\> Install Packages
-   Laden über library(NAME)
-   tidyverse und nycflights23

## Rohdaten transformieren und zusammenfassen

Beschreiben Sie die Schritte, um eine Tüte Gummibärchen nach Farben zu sortieren

## Rohdaten transformieren und zusammenfassen

![](images/gummibaerchen.png)

-   Öffne die Gummibärchentüte
-   Gruppiere die Gummibärchen nach Farbe
-   Zähle die Gummibärchen pro Farbe durch
-   Zugabe: Sortiere absteigend nach Häufigkeit

## Rohdaten transformieren und zusammenfassen

``` r
Gummibärchentüte %>%
  group_by(farbe) %>%
  summarize(Anzahl = n()) %>%
  arrange(desc(Anzahl))
```

## Typische Funktionen in Tidyverse

-   `group_by()`: Gruppieren nach Variablen
-   `select()`: Spalten auswählen
-   `filter()`: Zeilen nach Werten filtern
-   `mutate()`: Variablen erstellen oder modifizieren
-   `summarise()`: Mehrere Werte zu einem Ergebnis reduzieren
-   `arrange()`: Sortierung ändern

## Ausprobieren mit nycflights23

-   5 Datensätze
    -   flights
    -   planes
    -   airlines
    -   airports
    -   weather

## Demo

## Typische Fehler

-   Package nicht geladen
-   Pipe-Symbol falsch verwendet
-   Unvollständige Code-Chunks

## Datensätze miteinander verbinden

![](images/joins.png)

## Daten lesen und schreiben

-   Dateiformate: CSV, JSON, ARFF, Excel, Parquet, u.a.
-   Was sind die Vor- und Nachteile der Formate?
-   Import-Beispiele

## Pro-Tipp

-   Importzeile aus Code Preview kopieren und ins Notebook kopieren
-   library ignorieren (haben wir schon mit dem Tidyverse importiert)
-   View(gadata) ignorieren (nervt nur)

# Sitzung 4: Datenvisualisierung

## Warum Datenvisualisierung?

The greatest value of a picture is when it forces us to notice what we never expected to see. (John Tukey)

## Warum Datenvisualisierung, Teil 2

![](images/Anscombe's_quartet_3.svg.png)

## 5 wichtige Schritte

1.  Was ist der Kontext?
    -   An wen wird kommuniziert?
    -   Was soll der Empfänger wissen oder tun?
    -   Welche Daten sind vorhanden, um den Case zu untermauern?

## 5 wichtige Schritte

2.  Welche ist das passendste Visualierung?
    -   Ein oder zwei Zahlen, die im Fokus stehen? =\> Text!
    -   Liniendiagramme für continuous data
    -   Säulendiagramme für kategoriale Daten, müssen bei 0 anfangen
    -   Die Beziehungen der Daten untereinander bestimmen die Visualisierung, sofern Sie mehr als eine Variable haben
    -   Vermeiden Sie Tortendiagramme, Donuts, 3D, zweite Y-Achsen

## 5 wichtige Schritte

3.  Entfernen Sie „Clutter"
    1.  Entfernen Sie alles, das keinen informativen Mehrwert hat (z.B. Farben für die Ästhetik)
    2.  Arbeiten Sie mit freiem Raum

## 5 wichtige Schritte

4.  Fokussieren Sie die Aufmerksamkeit, wo Sie hin soll
    -   Größe, Position und, ja, manchmal auch Farbe nutzen um zu signalisieren, was wichtig ist
    -   Wohin gehen die Augen?

## 5 wichtige Schritte

::::: columns
::: {.column width="60%"}
5.  Nutzen Sie Storytelling
    -   Niemand will einfach nur Auflistungen von Zahlen und Diagrammen sehen.
    -   Schreiben Sie einen Plot. Anfang, eventuell sogar einen Twist, und ein Ende mit Call to Action (in der Wissenschaft reicht eine Diskussion ☺)
    -   Bitte arbeiten Sie nicht einfach nur mechanisch alles ab, was Sie können.
:::

::: {.column width="40%"}
![](images/storytelling.png)
:::
:::::


## Redundant Coding

::::: columns
::: {.column width="50%"}
![](images/redudant_coding1.png)
:::

::: {.column width="50%"}
![](images/redudant_coding2.png)
:::
:::::

## Weißer Space ist gut… aber nicht zu viel!



::::: columns
::: {.column width="50%"}
![](images/white_space1.png)
:::

::: {.column width="50%"}
![](images/white_space2.png)
:::
:::::

## Highlighting

![](images/highlighting.png)

## Hatten wir diese Frage nicht schon mal?

![](images/highlighting.png)

## Wie identifiziert man das richtige Diagramm?

![](images/overview.png)

## Scatter Plot / Streudiagramm

::::: columns
::: {.column width="60%"}
-   Zeigt Beziehung zwischen zwei kontinuierlichen Variablen
-   Jeder Punkt = eine Beobachtung
-   Erkennt Muster, Cluster, Ausreißer und Korrelationen
-   Benötigt numerische X/Y-Achsen
-   Dritte Dimension durch Punktgröße/Farbe möglich
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(42)
scatter_data <- data.frame(
  x = rnorm(100),
  y = rnorm(100),
  gruppe = sample(LETTERS[1:3], 100, replace = TRUE)
)
# Korrelation hinzufügen
scatter_data$y <- scatter_data$y + scatter_data$x - 0.7 + rnorm(100, 0, 0.5)
ggplot(scatter_data, aes(x = x, y = y, color = gruppe)) +
  geom_point(size = 3, alpha = 0.7) +
  scale_color_brewer(palette = "Set1") +
  theme_minimal() +
  labs(x = "Variable X", y = "Variable Y", color = "Gruppe")
```
:::
:::::

## Korrelationsmatrix

::::: columns
::: {.column width="60%"}
-   Zeigt Korrelationsstärke zwischen Variablen
-   Farbe/Größe visualisiert Richtung und Stärke
-   Unterscheidet klar positive/negative Korrelationen
-   Identifiziert Muster und redundante Variablen
-   Essentiell für explorative Datenanalyse
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
library(reshape2)

# Direkt eine gültige Korrelationsmatrix erstellen
set.seed(123)
vars <- 5
varNames <- LETTERS[1:vars]

# Simulierte Daten direkt erstellen
sim_data <- data.frame(
  A = rnorm(100),
  B = rnorm(100),
  C = rnorm(100),
  D = rnorm(100),
  E = rnorm(100)
)
# Korrelationen hinzufügen
sim_data$B <- sim_data$B + sim_data$A - 0.7 + rnorm(100, 0, 0.5)
sim_data$C <- sim_data$C - sim_data$A - 0.4 + rnorm(100, 0, 0.5)
sim_data$D <- sim_data$D + sim_data$B - 0.6 + rnorm(100, 0, 0.5)
sim_data$E <- sim_data$E - sim_data$C - 0.5 + sim_data$D - 0.3 + rnorm(100, 0, 0.5)

# Korrelationsmatrix berechnen
cormat <- round(cor(sim_data), 2)
melted_cormat <- melt(cormat)

ggplot(melted_cormat, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "firebrick", high = "steelblue", mid = "white", 
                     midpoint = 0, limit = c(-1,1)) +
  geom_text(aes(label = round(value, 2)), color = "black", size = 3) +
  theme_minimal() +
  coord_equal() +
  labs(fill = "Korrelation")
```
:::
:::::

## Venn-Diagramm

::::: columns
::: {.column width="60%"}
-   Zeigt gemeinsame und einzigartige Elemente
-   Überschneidungen visualisieren Gemeinsamkeiten
-   Ideal für Vergleich von Kategorien/Merkmalen
-   Funktioniert am besten mit 2-3 Gruppen
-   Wird mit mehr als 5 Gruppen unübersichtlich
:::

::: {.column width="40%"}
![](images/relationship_venn_4_3.png)
:::
:::::

## UpSet Diagram

::::: columns
::: {.column width="60%"}
-   Alternative für komplexe Überschneidungen
-   Skalierbarer als klassische Venn-Diagramme
-   Balken visualisieren Überschneidungsgrößen
-   Punkte zeigen beteiligte Mengen
-   Ideal für 4+ Gruppen/komplexe Beziehungen
:::

::: {.column width="40%"}
![](images/upset.png)
:::
:::::

## Sankey Diagram

::::: columns
::: {.column width="60%"}
-   Visualisiert Flüsse zwischen Kategorien
-   Pfeilbreite zeigt Volumen/Wert
-   Ideal für Verteilungen und Prozesse
-   Zeigt Herkunft und Ziel von Ressourcen
-   Verdeutlicht Proportionen über mehrere Stufen
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
library(ggalluvial)
# Sankey/Alluvial Diagramm
sankey_data <- data.frame(
  Quelle = rep(c("A", "B", "C"), times = c(3, 2, 2)),
  Ziel = c("X", "Y", "Z", "X", "Y", "Y", "Z"),
  Wert = c(5, 3, 2, 2, 3, 1, 4)
)
ggplot(sankey_data, aes(axis1 = Quelle, axis2 = Ziel, y = Wert)) +
  geom_alluvium(aes(fill = Quelle), width = 1/3) +
  geom_stratum(width = 1/3, fill = "grey80", color = "grey50") +
  geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
  scale_fill_brewer(palette = "Set1") +
  theme_minimal() +
  labs(x = NULL, y = NULL) +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks = element_blank())
```
:::
:::::

## Horizontal Bar Chart

::::: columns
::: {.column width="60%"}
-   Vergleicht Werte über Kategorien hinweg
-   Horizontale Anordnung für bessere Lesbarkeit
-   Ideal für lange Namen oder viele Kategorien
-   Perfekt für Rankings und Wertvergleiche
-   Einfach und übersichtlich
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
hbar_data <- data.frame(
  kategorie = c("Kategorie mit sehr langem Namen A", 
                "Kategorie B", 
                "Kategorie C", 
                "Kategorie D", 
                "Kategorie E"),
  wert = c(85, 72, 56, 41, 25)
)
ggplot(hbar_data, aes(x = reorder(kategorie, wert), y = wert)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(x = NULL, y = "Wert") +
  geom_text(aes(label = wert), hjust = -0.2) +
  ylim(0, max(hbar_data$wert) - 1.15)
```
:::
:::::

## Column Chart

::::: columns
::: {.column width="60%"}
-   Vergleicht Werte mit vertikalen Balken
-   Ideal für Ranking und Leistungsvergleiche
-   Funktioniert gut mit wenigen, klaren Kategorien
-   Kurze, lesbare Beschriftungen bevorzugen
-   Konsistente Nullbasislinie für faire Vergleiche
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
column_data <- data.frame(
  kategorie = factor(c("A", "B", "C", "D", "E"), levels = c("A", "B", "C", "D", "E")),
  wert = c(85, 72, 56, 41, 25)
)
ggplot(column_data, aes(x = kategorie, y = wert, fill = kategorie)) +
  geom_bar(stat = "identity", width = 0.7) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(x = NULL, y = "Wert") +
  theme(legend.position = "none") +
  geom_text(aes(label = wert), vjust = -0.5) +
  ylim(0, max(column_data$wert) - 1.1)
```
:::
:::::

## Vergleich: Horizontal vs. Vertical Bar Charts



| Feature | Horizontal | Vertical |
|------------------------|------------------------|------------------------|
| Best for | Long category labels, many items | Time-based or logical progression |
| Readability | Easier when labels are long | Requires rotation for long labels |
| Use case | Rankings, survey responses | Trends over time, grouped comparisons |
| Scan direction | Left to right | Bottom to top |
| Visual space | Efficient for many categories | Better for fewer, high-impact bars |

## Circular Bar Plot

::::: columns
::: {.column width="60%"}
-   Kreisförmige Darstellung von Werten mit Kategorien
-   Farbcodierung zeigt Gruppenzugehörigkeit
-   Kompakte Darstellung von Mustern und Beziehungen
-   Ideal für visuelles Storytelling
-   Beschriftungen sparsam und deutlich halten
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
library(dplyr)
data <- data.frame(
  category = rep(LETTERS[1:4], each = 5),
  subcategory = rep(1:5, 4),
  value = sample(20:100, 20)
)
ggplot(data, aes(x = subcategory, y = value, fill = category)) +
  geom_bar(stat = "identity", position = "dodge") +
  coord_polar() +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()
```
:::
:::::

## Diverging Bars

::::: columns
::: {.column width="60%"}
-   Zeigt Abweichungen in zwei Richtungen
-   Links/rechts von zentraler Basislinie
-   Ideal für Kontraste (positiv/negativ)
-   Zeigt Gleichgewicht vs. Ungleichgewicht
-   Gut für normalisierte Werte oder Z-Scores
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
diverging_data <- data.frame(
  category = factor(LETTERS[1:8], levels = LETTERS[8:1]),
  value = c(5, -2, 8, -3, 6, -1, 4, -7)
)
ggplot(diverging_data, aes(x = category, y = value, fill = value > 0)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = c("firebrick", "steelblue"), guide = "none") +
  coord_flip() +
  theme_minimal() +
  labs(y = "Abweichung vom Mittelwert")
```
:::
:::::

## Diverging Lollipop Chart

::::: columns
::: {.column width="60%"}
-   Elegantere Alternative zu Diverging Bars
-   Nutzt Punkte statt Balken (weniger Tinte)
-   Zeigt Richtung und Magnitude effektiv
-   Ideal wenn genaue Werte sekundär sind
-   Besonders gut für Präsentationen
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
lollipop_data <- data.frame(
  category = factor(LETTERS[1:8], levels = LETTERS[8:1]),
  value = c(5, -2, 8, -3, 6, -1, 4, -7)
)
ggplot(lollipop_data, aes(x = category, y = value, color = value > 0)) +
  geom_segment(aes(y = 0, x = category, yend = value, xend = category), color = "grey") +
  geom_point(size = 3) +
  scale_color_manual(values = c("firebrick", "steelblue"), guide = "none") +
  coord_flip() +
  theme_minimal() +
  labs(y = "Abweichung vom Mittelwert")
```
:::
:::::

## Histogram

::::: columns
::: {.column width="60%"}
-   Zeigt Verteilung numerischer Daten in Bins
-   Balken zeigen Häufigkeiten in Wertebereichen
-   Erkennt Schiefe, Streuung und Ausreißer
-   Zeigt Modi und Clustering in Daten
-   Berührende Balken betonen Kontinuität
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(123)
hist_data <- data.frame(
  value = c(rnorm(400, mean = 5, sd = 1), rnorm(100, mean = 8, sd = 0.5))
)
ggplot(hist_data, aes(x = value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  theme_minimal() +
  labs(y = "Häufigkeit")
```
:::
:::::

## Boxplots

::::: columns
::: {.column width="60%"}
-   Zeigt Minimum, Quartile und Median
-   Kompakte Darstellung von Verteilungen
-   Hebt Streuung und Ausreißer hervor
-   Ideal zum Kategorienvergleich
-   Kann für Laien schwer verständlich sein
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(123)
box_data <- data.frame(
  category = rep(c("A", "B", "C", "D"), each = 100),
  value = c(
    rnorm(100, mean = 5, sd = 1),
    rnorm(100, mean = 7, sd = 1.5),
    rnorm(100, mean = 4, sd = 0.8),
    rnorm(100, mean = 6, sd = 2)
  )
)
ggplot(box_data, aes(x = category, y = value, fill = category)) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Set2", guide = "none") +
  theme_minimal()
```
:::
:::::

## Boxplots mit Violin Plot

::::: columns
::: {.column width="60%"}
-   Kombiniert Boxplot mit Dichteverteilung
-   Zeigt Form, Streuung und zentrale Tendenz
-   Ideal für Gruppenvergleiche
-   Verdeutlicht Schiefe und Multimodalität
-   Informationsreichere Alternative zum Boxplot
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(123)
violin_data <- data.frame(
  category = rep(c("A", "B", "C"), each = 100),
  value = c(
    c(rnorm(50, 5, 1), rnorm(50, 8, 0.5)),
    rnorm(100, 7, 1.5),
    rnorm(100, 6, 0.7)
  )
)
ggplot(violin_data, aes(x = category, y = value, fill = category)) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.2, fill = "white", alpha = 0.7) +
  scale_fill_brewer(palette = "Set2", guide = "none") +
  theme_minimal()
```
:::
:::::

## Choropleth Map

::::: columns
::: {.column width="60%"}
-   Färbt Regionen basierend auf Datenwerten
-   Zeigt räumliche Muster und Unterschiede
-   Gut für Verhältnisse und Prozentsätze
-   Nutze normalisierte Daten (pro Kopf, %)
-   Vorsicht bei großen Flächenunterschieden
:::

::: {.column width="40%"}
![](images/map.png)
:::
:::::

## Stacked Area Chart

::::: columns
::: {.column width="60%"}
-   Zeigt Kategorienbeiträge im Zeitverlauf
-   Gestapelte Schichten zeigen Gesamtsumme
-   Gut für Wachstum und Anteile
-   Funktioniert am besten mit wenigen Kategorien
-   Einzelsegmentvergleich schwierig
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(42)
years <- 2010:2020
area_data <- data.frame(
  year = rep(years, 4),
  category = rep(c("A", "B", "C", "D"), each = length(years)),
  value = c(
    10:20 + rnorm(11, 0, 1),
    20:30 - rnorm(11, 0, 1),
    15:25 + rnorm(11, 0, 2),
    25:35 + rnorm(11, 0, 1.5)
  )
)
ggplot(area_data, aes(x = year, y = value, fill = category)) +
  geom_area() +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal()
```
:::
:::::

## Stacked 100% Area Chart

::::: columns
::: {.column width="60%"}
-   Zeigt relative Anteile, nicht absolute Werte
-   Verfolgt Änderungen von Kategorienanteilen
-   Summe immer 100% für leichte Vergleichbarkeit
-   Fokus auf Zusammensetzung, nicht Wachstum
-   Absolute Trends gehen verloren
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(42)
years <- 2010:2020
area100_data <- data.frame(
  year = rep(years, 4),
  category = rep(c("A", "B", "C", "D"), each = length(years)),
  value = c(
    10:20 + rnorm(11, 0, 1),
    20:30 - rnorm(11, 0, 1),
    15:25 + rnorm(11, 0, 2),
    25:35 + rnorm(11, 0, 1.5)
  )
)
ggplot(area100_data, aes(x = year, y = value, fill = category)) +
  geom_area(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  scale_fill_brewer(palette = "Set2") +
  theme_minimal() +
  labs(y = "Anteil")
```
:::
:::::

## Waterfall Diagram

::::: columns
::: {.column width="60%"}
-   Zeigt Beiträge zu einer Gesamtsumme
-   Visualisiert Zu- und Abnahmen über Kategorien
-   Hebt schrittweise Veränderungen hervor
-   Ideal für Gewinn- oder Budgetanalysen
-   Zeigt Faktoren für das Endergebnis
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
waterfall_data <- data.frame(
  stage = factor(c("Start", "Einnahmen", "Kosten", "Steuern", "Ende"), 
                levels = c("Start", "Einnahmen", "Kosten", "Steuern", "Ende")),
  value = c(0, 50, -30, -10, 0),
  end = c(100, 150, 120, 110, 110),
  start = c(100, 100, 150, 120, 110),
  type = c("total", "in", "out", "out", "total")
)
ggplot(waterfall_data, aes(x = stage, y = value, fill = type)) +
  geom_rect(aes(x = stage, xmin = as.numeric(stage) - 0.4, 
                xmax = as.numeric(stage) + 0.4, 
                ymin = pmin(start, end), ymax = pmax(start, end)), color = "black") +
  scale_fill_manual(values = c("in" = "steelblue", "out" = "firebrick", "total" = "darkgrey"), 
                    guide = "none") +
  theme_minimal() +
  labs(y = "Wert")
```
:::
:::::

## Heatmap

::::: columns
::: {.column width="60%"}
-   Nutzt Farbintensität für Matrixwerte
-   Zeigt Muster, Cluster und Anomalien
-   Gut für Korrelationen und Zeitreihen
-   Einfach zu überblicken, weniger präzise
-   Farbskala entscheidend für Interpretation
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
set.seed(42)
n <- 10
heatmap_data <- expand.grid(x = 1:n, y = 1:n)
heatmap_data$value <- matrix(rnorm(n^2), n, n)[cbind(heatmap_data$x, heatmap_data$y)]
ggplot(heatmap_data, aes(x = x, y = y, fill = value)) +
  geom_tile() +
  scale_fill_viridis_c() +
  theme_minimal() +
  coord_equal() +
  labs(x = "", y = "")
```
:::
:::::

## Square Area Chart

::::: columns
::: {.column width="60%"}
-   Zeigt Anteile mit gleichgroßen Quadraten
-   Typisch: 1 Quadrat = 1% (10×10-Raster)
-   Ideal für einfache Proportionsdarstellung
-   Leicht lesbar und ansprechend
-   Am besten mit wenigen Kategorien
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
square_data <- data.frame(
  x = rep(1:10, 10),
  y = rep(1:10, each = 10),
  category = c(rep("A", 40), rep("B", 25), rep("C", 20), rep("D", 15))
)
ggplot(square_data, aes(x = x, y = y, fill = category)) +
  geom_tile(color = "white") +
  scale_fill_brewer(palette = "Set2") +
  coord_equal() +
  theme_void() +
  labs(fill = "Kategorie")
```
:::
:::::

## Treemap

::::: columns
::: {.column width="60%"}
-   Zeigt Teil-Ganzes-Beziehungen mit Rechtecken
-   Größe entspricht dem Wert (Umsatz, Anzahl)
-   Visualisiert Hierarchien und Gruppierungen
-   Gut für Proportionsvergleiche
-   Vorsicht bei zu vielen kleinen Elementen
:::

::: {.column width="40%"}
```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 5
library(ggplot2)
library(treemapify)
treemap_data <- data.frame(
  category = c("A", "B", "C", "D", "E", "F"),
  subcategory = c("A1", "B1", "C1", "D1", "E1", "F1"),
  value = c(25, 18, 12, 10, 8, 5)
)
ggplot(treemap_data, aes(area = value, fill = category, label = subcategory)) +
  geom_treemap() +
  geom_treemap_text(color = "white", fontface = "bold") +
  scale_fill_brewer(palette = "Set2") +
  theme_void()
```
:::
:::::

## Treemap

![](images/treemap.png)

## Nein, Sie verwenden bitte keine Pie Charts!

![](images/pie-chart.png)



## Datenvisualisierung mit ggplot2

-   Base Plot vs. ggplot2
-   Grammar of Graphics
-   `geom_point()`, `geom_histogram()`, `geom_boxplot()`
-   Ästhetiken: Farbe, Form, Gruppierung


# Sitzung 5: Wahrscheinlichkeiten & Verteilungen

## Einführung in Wahrscheinlichkeiten
- **Was ist Zufall?** Konzeptuelle Einführung 
- **Laplace-Wahrscheinlichkeit**: Anzahl günstiger/Anzahl möglicher Fälle
- **Empirische Wahrscheinlichkeit**: Relative Häufigkeit bei Wiederholung
- **Bedingte Wahrscheinlichkeit**: P(A|B) - Wahrscheinlichkeit von A unter der Bedingung B

## Zufallsvariablen und Verteilungen
- **Diskrete vs. stetige Zufallsvariablen**: Konzeptioneller Unterschied
- **Wichtige diskrete Verteilungen**:
  - Binomialverteilung (Anzahl Erfolge bei n Versuchen)
  - Poisson-Verteilung (Anzahl Ereignisse in Zeitintervall)
- **Wichtige stetige Verteilungen**:
  - Gleichverteilung (konstante Wahrscheinlichkeitsdichte)
  - Normalverteilung (Gaußsche Glockenkurve)

## Normalverteilung im Detail
- **Eigenschaften**: Symmetrie, Mittelwert = Median = Modus
- **Standardnormalverteilung**: Z-Transformation
- **68-95-99,7 Regel** (Ein-, Zwei-, Dreisigma-Regel)
- **Praktische Anwendungen**: Warum viele natürliche Phänomene normalverteilt sind

## Zentraler Grenzwertsatz
- **Konzept**: Warum Stichprobenmittelwerte zur Normalverteilung tendieren
- **Demonstration durch Simulation in R**:
  ```r
  # Simulation des Zentralen Grenzwertsatzes
  n_samples <- 1000
  sample_means <- numeric(n_samples)
  
  for(i in 1:n_samples) {
    # Ziehe 30 Werte aus einer nicht-normalverteilten Grundgesamtheit
    sample_data <- runif(30, min=0, max=10)  # Gleichverteilung
    sample_means[i] <- mean(sample_data)
  }
  
  # Visualisierung der Verteilung der Stichprobenmittelwerte
  hist(sample_means, breaks=30, main="Verteilung der Stichprobenmittelwerte",
       xlab="Mittelwert", probability=TRUE)
  curve(dnorm(x, mean=mean(sample_means), sd=sd(sample_means)), 
        add=TRUE, col="red", lwd=2)
  ```

## Konzept des Standardfehlers
- **Definition**: Standardabweichung der Stichprobenmittelwerte
- **Formel**: $SE = \frac{\sigma}{\sqrt{n}}$
- **Bedeutung**: Grundlage für Konfidenzintervalle und Hypothesentests

## Praktische Anwendung in R

- **Funktionen für Wahrscheinlichkeitsverteilungen**:
  - `rnorm()`, `dnorm()`, `pnorm()`, `qnorm()`
  - Ebenso für andere Verteilungen (`rbinom`, `rpois`, etc.)
- **Visualisierung von Wahrscheinlichkeitsdichten**:

  # Normalverteilungen mit unterschiedlichen Parametern
  x <- seq(-4, 4, length.out = 1000)
  y1 <- dnorm(x, mean = 0, sd = 0.5)
  y2 <- dnorm(x, mean = 0, sd = 1)
  y3 <- dnorm(x, mean = 0, sd = 2)
  
  plot(x, y2, type="l", lwd=2, col="red", 
       main="Normalverteilungen mit unterschiedlichen Standardabweichungen",
       xlab="x", ylab="Wahrscheinlichkeitsdichte")
  lines(x, y1, lwd=2, col="blue")
  lines(x, y3, lwd=2, col="green")
  legend("topright", legend=c("σ = 0.5", "σ = 1", "σ = 2"),
         col=c("blue", "red", "green"), lwd=2)


# Sitzung 6: Einführung in die Inferenzstatistik

## Grundkonzepte der statistischen Inferenz
- **Von der Stichprobe zur Grundgesamtheit**: Warum Inferenzstatistik?
- **Punktschätzung vs. Intervallschätzung**: Einzelwerte vs. Vertrauensbereiche
- **Statistische Unsicherheit**: Warum exakte Rückschlüsse unmöglich sind

## Hypothesentesten: Grundlagen
- **Nullhypothese (H₀)**: Annahme keines Effekts/Unterschieds
- **Alternativhypothese (H₁)**: Annahme eines Effekts/Unterschieds
- **Arten von Hypothesen**:
  - Ungerichtete Hypothesen (zweiseitig): µ₁ ≠ µ₂
  - Gerichtete Hypothesen (einseitig): µ₁ > µ₂ oder µ₁ < µ₂

## Die Logik des Hypothesentestens
- **Indirekter Beweis**: Wir falsifizieren H₀ statt H₁ zu beweisen
- **Analogie zum Gerichtsprozess**: "Unschuldig bis zum Beweis der Schuld"
- **Entscheidungsbaum**: Wann wird H₀ beibehalten oder verworfen?

## Fehler beim Hypothesentesten
- **Fehler 1. Art (α-Fehler)**: Falsch-positive Entscheidung (H₀ fälschlicherweise verwerfen)
- **Fehler 2. Art (β-Fehler)**: Falsch-negative Entscheidung (H₀ fälschlicherweise beibehalten)
- **Power (1-β)**: Wahrscheinlichkeit, einen tatsächlich vorhandenen Effekt zu entdecken
- **Balanceakt**: Abwägung zwischen α und β

## Signifikanzniveau und p-Wert
- **Signifikanzniveau α**: Vorab festgelegte Irrtumswahrscheinlichkeit (typisch: 0.05)
- **p-Wert**: Wahrscheinlichkeit, die beobachteten (oder extremeren) Daten unter H₀ zu erhalten
- **Entscheidungsregel**: Verwerfe H₀, wenn p < α
- **Kritische Werte**: Alternative Darstellung der Entscheidungsregel

## Konfidenzintervalle
- **Definition**: Bereich, der mit einer bestimmten Wahrscheinlichkeit den wahren Parameter enthält
- **Interpretation**: Was bedeutet ein 95%-Konfidenzintervall?
- **Zusammenhang mit Hypothesentests**: Enthält ein 95%-KI den Wert der Nullhypothese?
- **Visualisierung von Konfidenzintervallen**:
  ```r
  # Beispiel: 95%-Konfidenzintervall für einen Mittelwert
  set.seed(123)
  sample_data <- rnorm(30, mean = 10, sd = 2)
  
  # t-Test für das Konfidenzintervall
  test_result <- t.test(sample_data)
  
  # Visualisierung
  plot(c(1), mean(sample_data), ylim=c(min(sample_data), max(sample_data)),
       pch=19, xlab="", ylab="Wert", xaxt="n", main="95%-Konfidenzintervall")
  axis(1, at=1, labels="Stichprobe")
  arrows(1, test_result$conf.int[1], 1, test_result$conf.int[2], 
         angle=90, code=3, length=0.1)
  abline(h=test_result$conf.int, lty=2, col="red")
  text(1.1, test_result$conf.int[1], round(test_result$conf.int[1], 2))
  text(1.1, test_result$conf.int[2], round(test_result$conf.int[2], 2))
  ```

## Experimentelles Design
- **Wichtige Konzepte**: Randomisierung, Kontrolle, Randomized Control Trial (RCT)
- **Experimentierfehler vermeiden**:
  - Placebo-Effekt
  - Hawthorne-Effekt
  - Observer Bias
  - Selection Bias
- **Die Statistik-Pille**: Praktisches Beispiel eines Experimentaufbaus
  - Testgruppe vs. Kontrollgruppe
  - Blindtest vs. Doppelblindtest

# Sitzung 7: t-Tests

### Historischer Kontext und Entwicklung
- **William Sealy Gosset und Guinness**: Ein Statistiker in der Brauerei
- **Warum "Student's t-Test"?**: Das Pseudonym hinter der Methode
- **Entwicklung über die Zeit**: Vom originalen t-Test zu modernen Varianten

### Theoretische Grundlagen
- **Die t-Verteilung**: Ähnlichkeit zur Normalverteilung mit dickeren "Schwänzen"
- **Freiheitsgrade**: Wie sie die Form der t-Verteilung beeinflussen
- **Visualisierung unterschiedlicher t-Verteilungen**:
  ```r
  # Vergleich von t-Verteilungen mit unterschiedlichen Freiheitsgraden
  x <- seq(-4, 4, length.out = 1000)
  y_normal <- dnorm(x)
  y_t1 <- dt(x, df = 1)  # 1 Freiheitsgrad
  y_t5 <- dt(x, df = 5)  # 5 Freiheitsgrade
  y_t30 <- dt(x, df = 30)  # 30 Freiheitsgrade
  
  plot(x, y_normal, type="l", lwd=2, col="black", 
       main="t-Verteilungen im Vergleich zur Normalverteilung",
       xlab="x", ylab="Wahrscheinlichkeitsdichte")
  lines(x, y_t1, lwd=2, col="red")
  lines(x, y_t5, lwd=2, col="blue")
  lines(x, y_t30, lwd=2, col="green")
  legend("topright", 
         legend=c("Normalverteilung", "t (df=1)", "t (df=5)", "t (df=30)"),
         col=c("black", "red", "blue", "green"), lwd=2)
  ```

### Arten von t-Tests
- **Einstichproben-t-Test**: Vergleich mit bekanntem oder hypothetischem Wert
  - Anwendungsbeispiel: Entspricht der mittlere Blutzuckerwert einer Patientengruppe dem Normalwert?
  - Formel: $t = \frac{\bar{x} - \mu_0}{s / \sqrt{n}}$
  
- **Zweistichproben-t-Test (unabhängig)**: Vergleich zweier unabhängiger Gruppen
  - Anwendungsbeispiel: Unterscheiden sich die Testergebnisse zweier unterschiedlicher Lernmethoden?
  - Formel bei gleichen Varianzen: $t = \frac{\bar{x}_1 - \bar{x}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}$
  
- **Gepaarter t-Test (abhängig)**: Vergleich verbundener Messungen
  - Anwendungsbeispiel: Vor-Nach-Vergleich einer Intervention
  - Formel: $t = \frac{\bar{d}}{s_d / \sqrt{n}}$
  
- **Welch-Test**: Variante für ungleiche Varianzen
  - Anwendungsbeispiel: Wenn die Streuung in beiden Gruppen deutlich unterschiedlich ist
  - Angepasste Freiheitsgrade

### Voraussetzungen der t-Tests
- **Normalverteilung**: Robustheit bei Verletzungen
- **Varianzhomogenität**: Bedeutung und Tests (Levene-Test, F-Test)
- **Unabhängigkeit der Beobachtungen**: Kritisch für die Gültigkeit
- **Zentraler Grenzwertsatz als Rettungsanker**: Wann können wir uns darauf verlassen?

### Praktische Durchführung in R
- **Einstichproben-t-Test**:
  ```r
  # Hypothetischer Wert: 100
  t.test(daten$werte, mu = 100)
  ```

- **Zweistichproben-t-Test**:
  ```r
  # Unabhängige Gruppen, gleiche Varianzen
  t.test(werte ~ gruppe, data = daten, var.equal = TRUE)
  
  # Unabhängige Gruppen, ungleiche Varianzen (Welch-Test)
  t.test(werte ~ gruppe, data = daten)
  ```

- **Gepaarter t-Test**:
  ```r
  # Verbundene Messungen (z.B. vorher-nachher)
  t.test(daten$nachher, daten$vorher, paired = TRUE)
  ```

### Effektstärken für t-Tests
- **Cohen's d**: Standardisierte Mittelwertdifferenz
  - Kleine (d = 0.2), mittlere (d = 0.5) und große (d = 0.8) Effekte
  - Berechnung in R:
    ```r
    library(effsize)
    cohen.d(werte ~ gruppe, data = daten)
    ```

- **Hedges' g**: Korrektur für kleine Stichproben
- **Bedeutung der Effektstärke**: Praktische Relevanz vs. statistische Signifikanz

### Visualisierung von t-Test-Ergebnissen
- **Boxplots mit Signifikanzmarkierungen**:
  ```r
  library(ggplot2)
  ggplot(daten, aes(x = gruppe, y = werte, fill = gruppe)) +
    geom_boxplot() +
    stat_summary(fun = mean, geom = "point", shape = 18, size = 3) +
    theme_minimal() +
    labs(title = "Vergleich der Gruppenmittelwerte",
         subtitle = "t-Test: p < 0.05*")
  ```

- **Mittelwerte mit Konfidenzintervallen**:
  ```r
  ggplot(daten_summary, aes(x = gruppe, y = mittelwert, fill = gruppe)) +
    geom_bar(stat = "identity", position = position_dodge()) +
    geom_errorbar(aes(ymin = mittelwert - ci, ymax = mittelwert + ci),
                 width = 0.2, position = position_dodge(0.9)) +
    theme_minimal() +
    labs(title = "Mittelwerte mit 95%-Konfidenzintervallen")
  ```

### Interpretation und Berichterstattung
- **Korrekte Interpretation des p-Werts**: Häufige Fehlinterpretationen vermeiden
- **Angabe des Konfidenzintervalls**: Zusätzliche Information zur Präzision
- **Standardisierte Berichtsform**: APA-Style für wissenschaftliche Publikationen
- **Beispiel**: "Ein unabhängiger t-Test zeigte einen signifikanten Unterschied zwischen den Gruppen (t(28) = 2.45, p = 0.02, d = 0.89). Die Experimentalgruppe (M = 25.3, SD = 4.2) erzielte höhere Werte als die Kontrollgruppe (M = 21.7, SD = 3.8)."

## Sitzung 8: ANOVA und Umfragedesign

### ANOVA-Grundlagen
- **Vom t-Test zur ANOVA**: Natürliche Erweiterung für mehr als zwei Gruppen
- **Varianzzerlegung**: Zwischen- vs. Innerhalb-Gruppenvarianz
- **F-Verteilung und F-Statistik**: Verhältnis der Varianzen
- **Grundformel**: $F = \frac{\text{Varianz zwischen Gruppen}}{\text{Varianz innerhalb Gruppen}}$

### Einfaktorielle ANOVA
- **Voraussetzungen**:
  - Normalverteilung der Residuen
  - Homoskedastizität (Varianzhomogenität)
  - Unabhängigkeit der Beobachtungen
- **Durchführung in R**:
  ```r
  # Einfaktorielle ANOVA
  anova_result <- aov(werte ~ gruppe, data = daten)
  summary(anova_result)
  ```

### Post-hoc Tests
- **Warum Post-hoc Tests?**: ANOVA sagt nur, dass Unterschiede existieren
- **Tukey's HSD**: Häufig verwendeter Test für alle paarweisen Vergleiche
- **Bonferroni-Korrektur**: Kontrolle des Alpha-Fehlers bei multiplen Vergleichen
- **Durchführung in R**:
  ```r
  # Tukey's HSD
  TukeyHSD(anova_result)
  
  # Alternativ mit dem Paket 'multcomp'
  library(multcomp)
  post_hoc <- glht(anova_result, linfct = mcp(gruppe = "Tukey"))
  summary(post_hoc)
  ```

### Visualisierung von ANOVA-Ergebnissen
- **Boxplots mit Post-hoc-Informationen**:
  ```r
  library(ggplot2)
  ggplot(daten, aes(x = gruppe, y = werte, fill = gruppe)) +
    geom_boxplot() +
    theme_minimal() +
    labs(title = "Vergleich der Gruppen (ANOVA)",
         subtitle = "F(2, 57) = 14.3, p < 0.001")
  ```

- **Interaktionsplots (für mehrfaktorielle ANOVA)**:
  ```r
  # Für mehrfaktorielle ANOVA
  interaction.plot(daten$faktor1, daten$faktor2, daten$werte,
                  mean, xlab = "Faktor 1", ylab = "Mittelwert",
                  trace.label = "Faktor 2")
  ```

### Umfragen und Datenerhebung

#### Grundsätzliche Aspekte der Datenerhebung
- **Primärdaten vs. Sekundärdaten**: Vor- und Nachteile
- **Quantitative vs. qualitative Forschung**: Unterschiedliche Ansätze und Zielsetzungen
- **Mixed Methods**: Kombination verschiedener Ansätze

#### Umfragedesign
- **Fragentypen**:
  - Offene vs. geschlossene Fragen
  - Single-Choice vs. Multiple-Choice
  - Likert-Skalen: Abstufungen und Best Practices
- **Skalendesign**:
  - Balance: Gleiche Anzahl positiver und negativer Optionen
  - Neutraloption: Ja oder nein?
  - Skalenlänge: 5-, 7- oder 10-Punkt-Skalen

#### Typische Fehlerquellen und Verzerrungen
- **Stichprobenverzerrung (Sampling Bias)**:
  - Convenience Sampling
  - Self-Selection Bias
  - Non-Response Bias
- **Antwortverzerrungen**:
  - Social Desirability Bias
  - Acquiescence Bias (Ja-Sage-Tendenz)
  - Extremity Bias (Tendenz zu Extremantworten)
  - Central Tendency Bias (Tendenz zur Mitte)

#### Fallbeispiele von Verzerrungen
- **Survivorship Bias**: Beobachtungen der "Überlebenden"
- **Verzerrung durch fehlende Daten**: Systematische vs. zufällige fehlende Werte
- **Kulturelle Unterschiede in der Beantwortung**: Internationale Studien

#### Praktische Tipps für bessere Umfragen
- **Frageformulierung**: Klar, präzise, neutral
- **Umfragelänge**: Ermüdung und Abbruchquoten
- **Pretests durchführen**: Frühe Probleme erkennen
- **Datenqualitätsprüfung**: Validierung während und nach der Erhebung

#### Online-Tools und Software
- **Umfragetools**: SurveyMonkey, Google Forms, LimeSurvey, Qualtrics
- **Integration mit R**: Datenimport und -analyse

## Sitzung 9: Korrelationsanalyse

### Grundkonzepte der Korrelation
- **Definition**: Stärke und Richtung eines Zusammenhangs zwischen Variablen
- **Unterschied zu Kausalität**: Korrelation impliziert keine Ursache-Wirkungs-Beziehung
- **Gemeinsame Varianz**: Was der Korrelationskoeffizient wirklich misst

### Von der Kovarianz zur Korrelation
- **Kovarianz**: Unstandarisiertes Maß für gemeinsame Variation
  - Formel: $\text{Cov}(X,Y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$
- **Pearson-Korrelation**: Standardisierte Kovarianz
  - Formel: $r = \frac{\text{Cov}(X,Y)}{s_X \cdot s_Y}$
  - Wertebereich: -1 bis +1

### Pearson-Korrelation
- **Voraussetzungen**:
  - Lineare Beziehung
  - Normalverteilung der Variablen (für Hypothesentests)
  - Keine Ausreißer (starker Einfluss auf r)
- **Interpretation der Stärke**:
  - Schwach: |r| ≈ 0.1-0.3
  - Mittel: |r| ≈ 0.3-0.5
  - Stark: |r| > 0.5
- **Durchführung in R**:
  ```r
  # Pearson-Korrelation
  cor.test(daten$x, daten$y, method = "pearson")
  ```

### Alternative Korrelationsmaße
- **Spearman-Rangkorrelation**: Für ordinale Daten oder nicht-lineare monotone Beziehungen
  - Formel: Pearson-Korrelation der rangtransformierten Daten
  - Robuster gegenüber Ausreißern
  - R-Code: `cor.test(daten$x, daten$y, method = "spearman")`

- **Kendall's Tau**: Alternative rangbasierte Korrelation
  - Basiert auf konkordanten vs. diskordanten Paaren
  - Robuster bei kleinen Stichproben
  - R-Code: `cor.test(daten$x, daten$y, method = "kendall")`

### Visualisierung von Korrelationen
- **Streudiagramme (Scatterplots)**:
  ```r
  plot(daten$x, daten$y, main = "Streudiagramm", 
       xlab = "Variable X", ylab = "Variable Y")
  abline(lm(daten$y ~ daten$x), col = "red")  # Regressionslinie
  ```

- **Korrelationsmatrizen**:
  ```r
  # Einfache Korrelationsmatrix
  cor(daten[, c("var1", "var2", "var3", "var4")])
  
  # Visualisierung mit corrplot
  library(corrplot)
  corrplot(cor(daten[, c("var1", "var2", "var3", "var4")]), 
           method = "circle", type = "upper", 
           addCoef.col = "black", number.cex = 0.7)
  ```

### Fallstricke und Fehlinterpretationen
- **Scheinkorrelationen**: Effekt von Drittvariablen
- **Nichtlineare Beziehungen**: Wenn r = 0 trotz starkem Zusammenhang
- **Aggregationsprobleme**: Simpson's Paradoxon
- **Extremwerte und Ausreißer**: Starker Einfluss auf die Korrelation

### Anscombe's Quartett
- **Gleiche statistische Kennwerte, unterschiedliche Beziehungen**
- **Bedeutung der visuellen Inspektion**:
  ```r
  # Anscombe's Quartett in R demonstrieren
  library(datasauRus)
  data(anscombe)
  
  par(mfrow = c(2, 2))
  for(i in 1:4) {
    plot(anscombe[, i], anscombe[, i+4], 
         main = paste("Datensatz", i),
         xlab = "x", ylab = "y")
    abline(lm(anscombe[, i+4] ~ anscombe[, i]), col = "red")
  }
  ```

### Inter-Rater Reliability
- **Definition**: Konsistenz zwischen verschiedenen Beurteilern
- **Anwendungsbereiche**: Subjektive Bewertungen, medizinische Diagnosen, Content-Analysen
- **Maße für kategoriale Daten**:
  - Cohen's Kappa (zwei Rater)
  - Fleiss' Kappa (mehr als zwei Rater)
- **Maße für kontinuierliche Daten**:
  - Intraclass Correlation Coefficient (ICC)
- **Durchführung in R**:

  # Cohen's Kappa für zwei Rater
  library(irr)
  kappa2(ratings[, c("rater1", "rater2")])
  
  # ICC für kontinuierliche Bewertungen
  icc(ratings[, c("rater1", "rater2", "rater3")])

- **Interpretation**: Abstufungen von "schlecht" bis "ausgezeichnet"

# Sitzung 10: Einfache Lineare Regression

## Von der Korrelation zur Regression
- **Unterschied**: Korrelation beschreibt Zusammenhang, Regression ermöglicht Vorhersagen
- **Abhängige vs. unabhängige Variable**: Y als Funktion von X
- **Lineare Beziehung**: Modellierung durch eine Gerade

## Das lineare Regressionsmodell
- **Grundgleichung**: $Y = \beta_0 + \beta_1 X + \varepsilon$
- **Parameter**:
  - Achsenabschnitt (Intercept) $\beta_0$: Wert von Y, wenn X = 0
  - Steigung (Slope) $\beta_1$: Änderung in Y bei Erhöhung von X um eine Einheit
  - Fehlerterm $\varepsilon$: Zufällige Abweichung (normalverteilt mit Mittelwert 0)
- **Kleinstquadrateschätzung**: Minimierung der quadrierten Residuen

## Durchführung in R
- **Einfache lineare Regression**:

  # Modell erstellen
  model <- lm(y ~ x, data = daten)
  
  # Zusammenfassung anzeigen
  summary(model)
  
  # Diagnostische Plots
  par(mfrow = c(2, 2))
  plot(model)


## Interpretation der Ergebnisse
- **Regressionskoeffizienten**: Bedeutung von Intercept und Slope
- **Standardfehler der Koeffizienten**: Präzision der Schätzung
- **t-Werte und p-Werte**: Signifikanz der Koeffizienten
- **R²**: Erklärte Varianz bzw. Bestimmtheitsmaß
  - Interpretation: Anteil der Varianz in Y, der durch X erklärt wird
  - Adjustiertes R²: Korrektur für die Anzahl der Prädiktoren

## Voraussetzungen und Diagnostik
- **Linearität**: Beziehung sollte linear sein
- **Normalverteilung der Residuen**: QQ-Plot zur Überprüfung
- **Homoskedastizität**: Konstante Varianz der Residuen
- **Unabhängigkeit der Beobachtungen**: Keine Autokorrelation
- **Ausreißer und einflussreiche Beobachtungen**: Cook's Distance, Leverage

## Visualisierung von Regressionsmodellen
- **Streudiagramm mit Regressionslinie**:

  library(ggplot2)
  ggplot(daten, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", se = TRUE) +
    theme_minimal() +
    labs(title = "Lineare Regression",
         subtitle = paste("R² =", round(summary(model)$r.squared, 3)))


- **Residuenplots**:

  # Residuen gegen vorhergesagte Werte
  ggplot(data.frame(fitted = fitted(model), residuals = residuals(model)), 
         aes(x = fitted, y = residuals)) +
    geom_point() +
    geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
    theme_minimal() +
    labs(title = "Residuen vs. vorhergesagte Werte")


## Praktische Anwendungsbeispiele

- **Fairer Preis für gebrauchte Artikel**: Preisvorhersage basierend auf Alter, Zustand, etc.
- **Verkaufsprognosen**: Vorhersage des Umsatzes basierend auf Werbeausgaben
- **Biometrische Zusammenhänge**: z.B. Zusammenhang zwischen Größe und Gewicht

## Transformation nicht-linearer Beziehungen
- **Logarithmische Transformation**: Für exponentielle Beziehungen
- **Quadratische Transformation**: Für U-förmige Beziehungen
- **Box-Cox-Transformation**: Systematische Suche nach optimaler Transformation
- **Beispiel in R**:
  ```r
  # Log-Transformation
  model_log <- lm(log(y) ~ x, data = daten)
  
  # Quadratische Transformation
  model_quad <- lm(y ~ x + I(x^2), data = daten)
  
  # Vergleich der Modelle mittels AIC
  AIC(model, model_log, model_quad)
  ```

## Logistische Regression
- **Unterschied zur linearen Regression**: Binäre abhängige Variable
- **Logit-Funktion**: Transformation von Wahrscheinlichkeiten in Log-Odds
- **Grundgleichung**: $\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X$
- **Interpretation**: Odds Ratios statt direkter Effekte

## Durchführung der logistischen Regression in R
- **Modellschätzung**:
  ```r
  # Logistische Regression
  logit_model <- glm(erfolg ~ x, data = daten, family = binomial)
  summary(logit_model)
  
  # Odds Ratios berechnen
  exp(coef(logit_model))
  ```

- **Vorhersagen und Evaluation**:
  ```r
  # Wahrscheinlichkeiten vorhersagen
  pred_probs <- predict(logit_model, type = "response")
  
  # ROC-Kurve und AUC
  library(pROC)
  roc_curve <- roc(daten$erfolg, pred_probs)
  plot(roc_curve)
  auc(roc_curve)
  ```

## Anwendungsbeispiele für logistische Regression
- **Medizin**: Vorhersage von Krankheitsrisiken
- **Marketing**: Conversion-Wahrscheinlichkeit von Kunden
- **Kreditwesen**: Ausfallwahrscheinlichkeit bei Krediten

# Sitzung 11: Chi-Quadrat-Test & Kategoriale Daten

## Einführung in kategoriale Datenanalyse
- **Charakteristika kategorialer Daten**: Nominal- und Ordinalskalen
- **Häufigkeitstabellen**: Absolute und relative Häufigkeiten
- **Kontingenztabellen**: Darstellung von Zusammenhängen zwischen kategorialen Variablen

## Der Chi-Quadrat-Test
- **Grundprinzip**: Vergleich beobachteter und erwarteter Häufigkeiten
- **Formel**: $\chi^2 = \sum \frac{(O - E)^2}{E}$
  - O: beobachtete Häufigkeit
  - E: erwartete Häufigkeit
- **Freiheitsgrade**: (Zeilen - 1) × (Spalten - 1)

## Arten von Chi-Quadrat-Tests
- **Anpassungstest (Goodness-of-Fit)**: Vergleich mit theoretischer Verteilung
  - Anwendung: Prüfung, ob Würfel fair ist
  - R-Code: `chisq.test(observed, p = expected_prob)`

- **Unabhängigkeitstest**: Zusammenhang zwischen zwei kategorialen Variablen
  - Anwendung: Zusammenhang zwischen Geschlecht und Studienfach
  - R-Code: `chisq.test(table(daten$var1, daten$var2))`

- **Homogenitätstest**: Vergleich von Verteilungen in verschiedenen Gruppen
  - Anwendung: Gleiche Verteilung einer Variablen in verschiedenen Populationen
  - R-Code: ähnlich zum Unabhängigkeitstest

## Voraussetzungen des Chi-Quadrat-Tests
- **Unabhängigkeit der Beobachtungen**: Keine wiederholten Messungen
- **Erwartete Häufigkeiten**: Idealerweise > 5 in jeder Zelle
- **Alternativen bei Verletzung der Voraussetzungen**:
  - Fisher's Exakter Test: Für kleine Stichproben
  - Yates-Korrektur: Für 2×2-Tabellen

## Durchführung in R
- **Einfacher Chi-Quadrat-Test**:
  ```r
  # Kontingenztabelle erstellen
  kontingenztabelle <- table(daten$kategorie1, daten$kategorie2)
  
  # Chi-Quadrat-Test durchführen
  chi_test <- chisq.test(kontingenztabelle)
  chi_test
  
  # Residuen anzeigen
  chi_test$residuals
  ```

## Effektstärken für Chi-Quadrat-Tests
- **Phi-Koeffizient**: Für 2×2-Tabellen
  - Formel: $\phi = \sqrt{\frac{\chi^2}{n}}$
  - R-Code: `sqrt(chi_test$statistic / sum(kontingenztabelle))`

- **Cramér's V**: Für größere Tabellen
  - Formel: $V = \sqrt{\frac{\chi^2}{n \times \min(r-1, c-1)}}$
  - R-Code: `library(vcd); assocstats(kontingenztabelle)`

## Visualisierung kategorialer Daten
- **Balkendiagramme**:
  ```r
  barplot(kontingenztabelle, beside = TRUE, 
          legend.text = TRUE, col = c("lightblue", "salmon"))
  ```

- **Mosaikplots**: Visualisierung von Kontingenztabellen
  ```r
  library(vcd)
  mosaic(kontingenztabelle, shade = TRUE, 
         legend = TRUE, labeling = labeling_border(rot_labels = c(45, 0, 0, 0)))
  ```

## Praktische Anwendungsbeispiele
- **Marktforschung**: Zusammenhänge zwischen demografischen Merkmalen und Kaufverhalten
- **Medizin**: Zusammenhänge zwischen Risikofaktoren und Krankheiten
- **Sozialforschung**: Analyse von Umfrageergebnissen

## Weitere Tests für kategoriale Daten
- **McNemar-Test**: Für abhängige Stichproben (vor-nach-Vergleich)
  - R-Code: `mcnemar.test(table(vor, nach))`

- **Cochran-Mantel-Haenszel-Test**: Für stratifizierte Tabellen
  - R-Code: `mantelhaen.test(x)`

- **Loglineare Modelle**: Für komplexe Zusammenhänge in mehrdimensionalen Tabellen
  - R-Code: `glm(count ~ var1 * var2, family = poisson, data = daten)`

## Analyse mit regulären Ausdrücken
- **Grundlegende reguläre Ausdrücke in R**:
  - `^` (Anfang), `




# Sitzung 12: Abschluss und Ausblick

## Zusammenfassung der wichtigsten Konzepte
- **Deskriptive Statistik**: Kennzahlen, Verteilungen, Visualisierung
- **Inferenzstatistik**: Stichproben, Hypothesentests, Konfidenzintervalle
- **Korrelation und Regression**: Zusammenhänge und Vorhersagen
- **Kategoriale Datenanalyse**: Chi-Quadrat-Tests und Kontingenztabellen

## Integration der Methoden an praktischen Beispielen
- **Fallstudie**: Durchführung einer vollständigen Datenanalyse
  1. Datenimport und -bereinigung
  2. Explorative Datenanalyse
  3. Hypothesenbildung
  4. Statistische Tests
  5. Modellierung (Regression)
  6. Interpretation und Berichterstattung

## Bewertungskriterien für die Hausarbeit
- **Datenauswahl und -aufbereitung**: Sinnvolle Auswahl, korrekte Aufbereitung
- **Methodik**: Angemessene Auswahl und korrekte Anwendung statistischer Verfahren
- **Visualisierung**: Informative, korrekte und ästhetisch ansprechende Grafiken
- **Interpretation**: Korrekte Deutung der Ergebnisse, Verständnis der Grenzen
- **Dokumentation**: Klare, nachvollziehbare Darstellung der Analyse mit R Markdown

## Weiterführende statistische Methoden
- **Mehrfaktorielle ANOVA**: Analyse mehrerer Einflussfaktoren und ihrer Interaktionen
- **Multiple Regression**: Mehrere unabhängige Variablen
- **Nicht-parametrische Verfahren**: Alternativen bei Verletzung der Normalverteilungsannahme
- **Zeitreihenanalyse**: Analyse von Daten mit zeitlicher Struktur
- **Faktorenanalyse**: Reduktion von Variablen auf latente Faktoren
- **Clusteranalyse**: Identifikation von Gruppen in Daten
- **Machine Learning**: Vorhersagemodelle und Klassifikation

## Praktische Hinweise für statistisches Arbeiten
- **Reproduzierbarkeit**: Verwendung von R Markdown für nachvollziehbare Analysen
- **Visualisierung vor Berechnung**: Daten immer zuerst visualisieren
- **Datenvalidierung**: Überprüfung auf Ausreißer, fehlende Werte, unmögliche Werte
- **Kritisches Denken**: Hinterfragen von Ergebnissen und Methoden
- **Effektgrößen beachten**: Nicht nur auf p-Werte fixieren

## Ressourcen für das Selbststudium
- **Bücher**:
  - "R for Data Science" von Hadley Wickham und Garrett Grolemund
  - "Discovering Statistics Using R" von Andy Field
  - "An Introduction to Statistical Learning" von James, Witten, Hastie und Tibshirani

- **Online-Ressourcen**:
  - RStudio Cheatsheets: https://www.rstudio.com/resources/cheatsheets/
  - R-bloggers: https://www.r-bloggers.com/
  - Datacamp-Kurse: https://www.datacamp.com/courses/tech:r
  - Stackoverflow für R-spezifische Fragen

## Datenkritisches Denken und ethische Aspekte
- **Datenqualität und -herkunft**: Kritische Bewertung von Datenquellen
- **Fallstricke der Statistik**: Häufige Fehlinterpretationen und wie man sie vermeidet
- **Ethik in der Datenanalyse**: Datenschutz, Fairness, Transparenz
- **Kommunikation statistischer Ergebnisse**: Verantwortungsvoller Umgang mit Daten in der Öffentlichkeit

## Abschlussdiskussion und Feedback
- **Offene Fragen**: Klärung verbleibender Unklarheiten
- **Rückblick**: Was waren die wichtigsten Erkenntnisse?
- **Ausblick**: Wie können die Studierenden die erlernten Methoden in ihren Abschlussarbeiten anwenden?
- **Kursevaluation**: Feedback zum Seminar und Verbesserungsvorschläge

## Tipp des Tages: Statistische Fallstricke vermeiden
- **Data Dredging (p-Hacking)**: Nicht so lange testen, bis ein signifikantes Ergebnis erscheint
- **HARKing (Hypothesizing After Results are Known)**: Hypothesen vor der Analyse festlegen
- **Cherry Picking**: Alle relevanten Ergebnisse berichten, nicht nur die "schönen"
- **Überinterpretation von p-Werten**: p < 0.05 bedeutet nicht "wahr" oder "wichtig"
- **Ignorieren von Effektgrößen**: Praktische Bedeutsamkeit ist oft wichtiger als statistische Signifikanz
