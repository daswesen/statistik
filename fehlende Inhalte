


# Sitzung 9: Einfache Lineare Regression

## Von der Korrelation zur Regression

-   **Unterschied**: Korrelation beschreibt Zusammenhang, Regression ermöglicht Vorhersagen
-   **Abhängige vs. unabhängige Variable**: Y als Funktion von X
-   **Lineare Beziehung**: Modellierung durch eine Gerade

## Das lineare Regressionsmodell

-   **Grundgleichung**: $Y = \beta_0 + \beta_1 X + \varepsilon$
-   **Parameter**:
    -   Achsenabschnitt (Intercept) $\beta_0$: Wert von Y, wenn X = 0
    -   Steigung (Slope) $\beta_1$: Änderung in Y bei Erhöhung von X um eine Einheit
    -   Fehlerterm $\varepsilon$: Zufällige Abweichung (normalverteilt mit Mittelwert 0)
-   **Kleinstquadrateschätzung**: Minimierung der quadrierten Residuen

## Durchführung in R

-   **Einfache lineare Regression**:

    # Modell erstellen

    model \<- lm(y \~ x, data = daten)

    # Zusammenfassung anzeigen

    summary(model)

    # Diagnostische Plots

    par(mfrow = c(2, 2)) plot(model)

## Interpretation der Ergebnisse

-   **Regressionskoeffizienten**: Bedeutung von Intercept und Slope
-   **Standardfehler der Koeffizienten**: Präzision der Schätzung
-   **t-Werte und p-Werte**: Signifikanz der Koeffizienten
-   **R²**: Erklärte Varianz bzw. Bestimmtheitsmaß
    -   Interpretation: Anteil der Varianz in Y, der durch X erklärt wird
    -   Adjustiertes R²: Korrektur für die Anzahl der Prädiktoren

## Voraussetzungen und Diagnostik

-   **Linearität**: Beziehung sollte linear sein
-   **Normalverteilung der Residuen**: QQ-Plot zur Überprüfung
-   **Homoskedastizität**: Konstante Varianz der Residuen
-   **Unabhängigkeit der Beobachtungen**: Keine Autokorrelation
-   **Ausreißer und einflussreiche Beobachtungen**: Cook's Distance, Leverage

## Visualisierung von Regressionsmodellen

-   **Streudiagramm mit Regressionslinie**:

    library(ggplot2) ggplot(daten, aes(x = x, y = y)) + geom_point() + geom_smooth(method = "lm", se = TRUE) + theme_minimal() + labs(title = "Lineare Regression", subtitle = paste("R² =", round(summary(model)\$r.squared, 3)))

-   **Residuenplots**:

    # Residuen gegen vorhergesagte Werte

    ggplot(data.frame(fitted = fitted(model), residuals = residuals(model)), aes(x = fitted, y = residuals)) + geom_point() + geom_hline(yintercept = 0, linetype = "dashed", color = "red") + theme_minimal() + labs(title = "Residuen vs. vorhergesagte Werte")

## Praktische Anwendungsbeispiele

-   **Fairer Preis für gebrauchte Artikel**: Preisvorhersage basierend auf Alter, Zustand, etc.
-   **Verkaufsprognosen**: Vorhersage des Umsatzes basierend auf Werbeausgaben
-   **Biometrische Zusammenhänge**: z.B. Zusammenhang zwischen Größe und Gewicht

## Transformation nicht-linearer Beziehungen

-   **Logarithmische Transformation**: Für exponentielle Beziehungen

-   **Quadratische Transformation**: Für U-förmige Beziehungen

-   **Box-Cox-Transformation**: Systematische Suche nach optimaler Transformation

-   **Beispiel in R**:

    ``` r
    # Log-Transformation
    model_log <- lm(log(y) ~ x, data = daten)

    # Quadratische Transformation
    model_quad <- lm(y ~ x + I(x^2), data = daten)

    # Vergleich der Modelle mittels AIC
    AIC(model, model_log, model_quad)
    

# Sitzung 10: Logistische Regression

-   **Unterschied zur linearen Regression**: Binäre abhängige Variable
-   **Logit-Funktion**: Transformation von Wahrscheinlichkeiten in Log-Odds
-   **Grundgleichung**: $\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X$
-   **Interpretation**: Odds Ratios statt direkter Effekte

## Durchführung der logistischen Regression in R

-   **Modellschätzung**:

    ``` r
    # Logistische Regression
    logit_model <- glm(erfolg ~ x, data = daten, family = binomial)
    summary(logit_model)

    # Odds Ratios berechnen
    exp(coef(logit_model))
    ```

-   **Vorhersagen und Evaluation**:

    ``` r
    # Wahrscheinlichkeiten vorhersagen
    pred_probs <- predict(logit_model, type = "response")

    # ROC-Kurve und AUC
    library(pROC)
    roc_curve <- roc(daten$erfolg, pred_probs)
    plot(roc_curve)
    auc(roc_curve)
    ```

## Anwendungsbeispiele für logistische Regression

-   **Medizin**: Vorhersage von Krankheitsrisiken
-   **Marketing**: Conversion-Wahrscheinlichkeit von Kunden
-   **Kreditwesen**: Ausfallwahrscheinlichkeit bei Krediten


# Sitzung 11: ANOVA und Umfragedesign

## ANOVA-Grundlagen

-   **Vom t-Test zur ANOVA**: Natürliche Erweiterung für mehr als zwei Gruppen
-   **Varianzzerlegung**: Zwischen- vs. Innerhalb-Gruppenvarianz
-   **F-Verteilung und F-Statistik**: Verhältnis der Varianzen
-   **Grundformel**: $F = \frac{\text{Varianz zwischen Gruppen}}{\text{Varianz innerhalb Gruppen}}$

## Einfaktorielle ANOVA

-   **Voraussetzungen**:

    -   Normalverteilung der Residuen
    -   Homoskedastizität (Varianzhomogenität)
    -   Unabhängigkeit der Beobachtungen

-   **Durchführung in R**:

    ``` r
    # Einfaktorielle ANOVA
    anova_result <- aov(werte ~ gruppe, data = daten)
    summary(anova_result)
    ```

## Post-hoc Tests

-   **Warum Post-hoc Tests?**: ANOVA sagt nur, dass Unterschiede existieren

-   **Tukey's HSD**: Häufig verwendeter Test für alle paarweisen Vergleiche

-   **Bonferroni-Korrektur**: Kontrolle des Alpha-Fehlers bei multiplen Vergleichen

-   **Durchführung in R**:

    ``` r
    # Tukey's HSD
    TukeyHSD(anova_result)

    # Alternativ mit dem Paket 'multcomp'
    library(multcomp)
    post_hoc <- glht(anova_result, linfct = mcp(gruppe = "Tukey"))
    summary(post_hoc)
    ```

## Visualisierung von ANOVA-Ergebnissen

-   **Boxplots mit Post-hoc-Informationen**:

    ``` r
    library(ggplot2)
    ggplot(daten, aes(x = gruppe, y = werte, fill = gruppe)) +
      geom_boxplot() +
      theme_minimal() +
      labs(title = "Vergleich der Gruppen (ANOVA)",
           subtitle = "F(2, 57) = 14.3, p < 0.001")
    ```

-   **Interaktionsplots (für mehrfaktorielle ANOVA)**:

    ``` r
    # Für mehrfaktorielle ANOVA
    interaction.plot(daten$faktor1, daten$faktor2, daten$werte,
                    mean, xlab = "Faktor 1", ylab = "Mittelwert",
                    trace.label = "Faktor 2")
    ```

## Umfragen und Datenerhebung

### Grundsätzliche Aspekte der Datenerhebung

-   **Primärdaten vs. Sekundärdaten**: Vor- und Nachteile
-   **Quantitative vs. qualitative Forschung**: Unterschiedliche Ansätze und Zielsetzungen
-   **Mixed Methods**: Kombination verschiedener Ansätze

### Umfragedesign

-   **Fragentypen**:
    -   Offene vs. geschlossene Fragen
    -   Single-Choice vs. Multiple-Choice
    -   Likert-Skalen: Abstufungen und Best Practices
-   **Skalendesign**:
    -   Balance: Gleiche Anzahl positiver und negativer Optionen
    -   Neutraloption: Ja oder nein?
    -   Skalenlänge: 5-, 7- oder 10-Punkt-Skalen

### Typische Fehlerquellen und Verzerrungen

-   **Stichprobenverzerrung (Sampling Bias)**:
    -   Convenience Sampling
    -   Self-Selection Bias
    -   Non-Response Bias
-   **Antwortverzerrungen**:
    -   Social Desirability Bias
    -   Acquiescence Bias (Ja-Sage-Tendenz)
    -   Extremity Bias (Tendenz zu Extremantworten)
    -   Central Tendency Bias (Tendenz zur Mitte)

### Fallbeispiele von Verzerrungen

-   **Survivorship Bias**: Beobachtungen der "Überlebenden"
-   **Verzerrung durch fehlende Daten**: Systematische vs. zufällige fehlende Werte
-   **Kulturelle Unterschiede in der Beantwortung**: Internationale Studien

### Praktische Tipps für bessere Umfragen

-   **Frageformulierung**: Klar, präzise, neutral
-   **Umfragelänge**: Ermüdung und Abbruchquoten
-   **Pretests durchführen**: Frühe Probleme erkennen
-   **Datenqualitätsprüfung**: Validierung während und nach der Erhebung

### Online-Tools und Software

-   **Umfragetools**: SurveyMonkey, Google Forms, LimeSurvey, Qualtrics
-   **Integration mit R**: Datenimport und -analyse



# Sitzung 12: Abschluss und Ausblick

## Zusammenfassung der wichtigsten Konzepte

-   **Deskriptive Statistik**: Kennzahlen, Verteilungen, Visualisierung
-   **Inferenzstatistik**: Stichproben, Hypothesentests, Konfidenzintervalle
-   **Korrelation und Regression**: Zusammenhänge und Vorhersagen
-   **Kategoriale Datenanalyse**: Chi-Quadrat-Tests und Kontingenztabellen

## Integration der Methoden an praktischen Beispielen

-   **Fallstudie**: Durchführung einer vollständigen Datenanalyse
    1.  Datenimport und -bereinigung
    2.  Explorative Datenanalyse
    3.  Hypothesenbildung
    4.  Statistische Tests
    5.  Modellierung (Regression)
    6.  Interpretation und Berichterstattung

## Bewertungskriterien für die Hausarbeit

-   **Datenauswahl und -aufbereitung**: Sinnvolle Auswahl, korrekte Aufbereitung
-   **Methodik**: Angemessene Auswahl und korrekte Anwendung statistischer Verfahren
-   **Visualisierung**: Informative, korrekte und ästhetisch ansprechende Grafiken
-   **Interpretation**: Korrekte Deutung der Ergebnisse, Verständnis der Grenzen
-   **Dokumentation**: Klare, nachvollziehbare Darstellung der Analyse mit R Markdown

## Weiterführende statistische Methoden

-   **Mehrfaktorielle ANOVA**: Analyse mehrerer Einflussfaktoren und ihrer Interaktionen
-   **Multiple Regression**: Mehrere unabhängige Variablen
-   **Nicht-parametrische Verfahren**: Alternativen bei Verletzung der Normalverteilungsannahme
-   **Zeitreihenanalyse**: Analyse von Daten mit zeitlicher Struktur
-   **Faktorenanalyse**: Reduktion von Variablen auf latente Faktoren
-   **Clusteranalyse**: Identifikation von Gruppen in Daten
-   **Machine Learning**: Vorhersagemodelle und Klassifikation

## Praktische Hinweise für statistisches Arbeiten

-   **Reproduzierbarkeit**: Verwendung von R Markdown für nachvollziehbare Analysen
-   **Visualisierung vor Berechnung**: Daten immer zuerst visualisieren
-   **Datenvalidierung**: Überprüfung auf Ausreißer, fehlende Werte, unmögliche Werte
-   **Kritisches Denken**: Hinterfragen von Ergebnissen und Methoden
-   **Effektgrößen beachten**: Nicht nur auf p-Werte fixieren

## Ressourcen für das Selbststudium

-   **Bücher**:
    -   "R for Data Science" von Hadley Wickham und Garrett Grolemund
    -   "Discovering Statistics Using R" von Andy Field
    -   "An Introduction to Statistical Learning" von James, Witten, Hastie und Tibshirani
-   **Online-Ressourcen**:
    -   RStudio Cheatsheets: https://www.rstudio.com/resources/cheatsheets/
    -   R-bloggers: https://www.r-bloggers.com/
    -   Datacamp-Kurse: https://www.datacamp.com/courses/tech:r
    -   Stackoverflow für R-spezifische Fragen

## Datenkritisches Denken und ethische Aspekte

-   **Datenqualität und -herkunft**: Kritische Bewertung von Datenquellen
-   **Fallstricke der Statistik**: Häufige Fehlinterpretationen und wie man sie vermeidet
-   **Ethik in der Datenanalyse**: Datenschutz, Fairness, Transparenz
-   **Kommunikation statistischer Ergebnisse**: Verantwortungsvoller Umgang mit Daten in der Öffentlichkeit

## Abschlussdiskussion und Feedback

-   **Offene Fragen**: Klärung verbleibender Unklarheiten
-   **Rückblick**: Was waren die wichtigsten Erkenntnisse?
-   **Ausblick**: Wie können die Studierenden die erlernten Methoden in ihren Abschlussarbeiten anwenden?
-   **Kursevaluation**: Feedback zum Seminar und Verbesserungsvorschläge

## Tipp des Tages: Statistische Fallstricke vermeiden

-   **Data Dredging (p-Hacking)**: Nicht so lange testen, bis ein signifikantes Ergebnis erscheint
-   **HARKing (Hypothesizing After Results are Known)**: Hypothesen vor der Analyse festlegen
-   **Cherry Picking**: Alle relevanten Ergebnisse berichten, nicht nur die "schönen"
-   **Überinterpretation von p-Werten**: p \< 0.05 bedeutet nicht "wahr" oder "wichtig"
-   **Ignorieren von Effektgrößen**: Praktische Bedeutsamkeit ist oft wichtiger als statistische Signifikanz
